{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_url = 'https://www.ku.ac.th/th/'\n",
    "frontier_q = [seed_url]\n",
    "visited_q = []\n",
    "visited_netloc = []\n",
    "success_html = []\n",
    "domain = 'ku.ac.th'\n",
    "EXCEPT_FILE = ('pdf', 'doc', 'xls', 'ppt', 'exe', 'jpg', 'mpg', 'zip')\n",
    "WEB_TYPE = (\"html\", \"htm\", \"asp\", \"php\")\n",
    "PAGE_COUNT = 0\n",
    "DEBUG = False\n",
    "LOG_WRITE = True\n",
    "LOG_READ = True\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'mayo_bot',\n",
    "    'From': 'ittiwat.ma@ku.th'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUG, READLOG, WRITELOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log(filename):\n",
    "    f = open('log/'+filename, 'r',encoding='utf-8')\n",
    "    return f.read()\n",
    "\n",
    "def log_to_array(raw_file):\n",
    "    return raw_file.split('\\n')\n",
    "\n",
    "def write_txt_log(list_string, filename, type = \"list\"):\n",
    "    os.makedirs(\"log\", 0o755, exist_ok=True)\n",
    "    if type == \"list\":\n",
    "        list_string = \"\\n\".join(list_string)\n",
    "    f = codecs.open(\"log/\"+filename+\".txt\", 'w', 'utf-8')\n",
    "    f.write(list_string)\n",
    "    f.close()\n",
    "    \n",
    "def write_log_file(f = 250):\n",
    "    if(PAGE_COUNT%f == 0):\n",
    "        write_txt_log(visited_netloc, \"visited_netloc\",)\n",
    "        write_txt_log(visited_q, \"visited_q\")\n",
    "        write_txt_log(success_html, \"success_html\")\n",
    "        write_txt_log(frontier_q, \"frontier_q\")\n",
    "        write_txt_log(str(PAGE_COUNT), \"PAGE_COUNT\", \"number\")\n",
    "\n",
    "def debug(url,type):\n",
    "    global PAGE_COUNT\n",
    "    try:\n",
    "        if('text/html' in type):\n",
    "            print('page: ',PAGE_COUNT)\n",
    "            print('site: ',url)\n",
    "            print(type)\n",
    "            print('Write success')\n",
    "            print('-'*50)\n",
    "        else:\n",
    "            print('site: ',url)\n",
    "            print(type)\n",
    "            print('-'*50)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "if LOG_READ:\n",
    "    try:\n",
    "        frontier_q = log_to_array(read_log('frontier_q.txt'))\n",
    "        visited_q = log_to_array(read_log('visited_q.txt'))\n",
    "        visited_netloc = log_to_array(read_log('visited_netloc.txt'))\n",
    "        success_html = log_to_array(read_log('success_html.txt'))\n",
    "        PAGE_COUNT = int(read_log('PAGE_COUNT.txt'))\n",
    "    except:\n",
    "        print('no log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE FILE HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, codecs\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "\n",
    "\n",
    "def write_html_with_path(url, raw_html):\n",
    "    path = ''\n",
    "    file_type = ''\n",
    "    abs_file = ''\n",
    "    link = urlparse(url)\n",
    "    netloc = link.netloc\n",
    "    path = link.path\n",
    "    \n",
    "    if(path.endswith(WEB_TYPE)):\n",
    "        file_path = 'html/' + netloc + '/'.join(path.split(\"/\")[:-1])+ \"/\"\n",
    "        abs_file = file_path  + path.split(\"/\")[-1]\n",
    "    else:\n",
    "        file_path = 'html/' + netloc + path + '/'\n",
    "        abs_file = file_path  + \"dummy\"\n",
    "    \n",
    "    os.makedirs(file_path, 0o755, exist_ok=True)\n",
    "    f = codecs.open(abs_file, 'w', 'utf-8')\n",
    "    f.write(raw_html)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "#get page html \n",
    "def get_page(url, robots = False):\n",
    "    global headers\n",
    "    global PAGE_COUNT\n",
    "    text = ''\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=2)\n",
    "        # If the response was successful, no Exception will be raised\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        if(DEBUG):\n",
    "            print(\"fail!\")\n",
    "            print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "            print('-'*50)\n",
    "    except Exception as err:\n",
    "        if(DEBUG):\n",
    "            print(\"fail!\")\n",
    "            print(f'Other error occurred: {err}')  # Python 3.6\n",
    "            print('-'*50)\n",
    "    else:\n",
    "        try:\n",
    "            if(robots == False):\n",
    "                if('text/html; charset=UTF-8' == response.headers['Content-Type'] ):\n",
    "                    PAGE_COUNT += 1\n",
    "                    success_html.append(url)\n",
    "                    response.encoding = 'utf-8'\n",
    "                    text = response.text    \n",
    "                    write_html_with_path(url,text)\n",
    "                    \n",
    "                    #visit netloc history\n",
    "                    link  = urlparse(url)\n",
    "                    link_netlock = link.scheme + \"://\" + link.netloc\n",
    "                    if link_netlock not in visited_netloc:\n",
    "                        visited_netloc.append(link_netlock)\n",
    "                if(DEBUG):\n",
    "                    debug(url,response.headers['Content-Type'])\n",
    "                if(LOG_WRITE):\n",
    "                    write_log_file()\n",
    "            if(robots):\n",
    "                if('text/plain' in response.headers['Content-Type']):\n",
    "                    text = response.text    \n",
    "        except:\n",
    "            pass\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find a href link in current url\n",
    "def get_all_link_form_url(current_url):\n",
    "    urls = []\n",
    "    raw_html = get_page(current_url)\n",
    "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
    "    index = 0;  length = len(raw_html)\n",
    "    while index < length:\n",
    "        start = raw_html.find(pattern_start, index)\n",
    "        if start > 0:\n",
    "            start = start + len(pattern_start)\n",
    "            end = raw_html.find(pattern_end, start)\n",
    "            link = raw_html[start:end]\n",
    "            if len(link) > 0:\n",
    "                if link not in urls:\n",
    "                    link = urljoin(current_url, link)\n",
    "                    url = urlparse(link)\n",
    "                    link = url.scheme + '://' + url.netloc + url.path\n",
    "                    urls.append(link)\n",
    "            index = end\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enqueue(links):\n",
    "    global frontier_q\n",
    "    for link in links:\n",
    "        if link not in frontier_q and link not in visited_q and domain in link and not link.endswith(EXCEPT_FILE) and 'mailto:/' not in link:\n",
    "            frontier_q.append(link)\n",
    "\n",
    "# FIFO queue\n",
    "def dequeue():\n",
    "    global frontier_q\n",
    "    current_url = frontier_q[0]\n",
    "    frontier_q = frontier_q[1:]\n",
    "    return current_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "while (PAGE_COUNT < 10001 and frontier_q):\n",
    "    current_url = dequeue()\n",
    "    current_url = unquote(current_url.strip())\n",
    "    visited_q.append(current_url)\n",
    "    extracted_links = get_all_link_form_url(current_url)\n",
    "    enqueue(extracted_links)\n",
    "    \n",
    "    \n",
    "    \n",
    "write_log_file(1)\n",
    "print(\"=\"*55)\n",
    "print(\"DONE !\"*9)\n",
    "print(\"=\"*55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROBOTS, SITEMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robots_web = []\n",
    "sitemap_web = []\n",
    "\n",
    "for site in visited_netloc:\n",
    "    t = get_page(site + '/robots.txt', True)\n",
    "    if(t):\n",
    "        robots_web.append('https://'+ urlparse(site).netloc)\n",
    "        if('sitemap:' in t):\n",
    "            sitemap_web.append('https://'+urlparse(site).netloc)\n",
    "\n",
    "f = codecs.open('list_robots.txt', 'w', 'utf-8')\n",
    "f.write('\\n'.join(robots_web))\n",
    "f.close()\n",
    "\n",
    "f = codecs.open('list_sitemap.txt', 'w', 'utf-8')\n",
    "f.write('\\n'.join(sitemap_web))\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0daeec5c9a0e7651df925a87f9f66bfbe54a6429e4bc0028c82b17a7896f4c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
